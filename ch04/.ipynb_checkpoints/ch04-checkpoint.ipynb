{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd44312",
   "metadata": {},
   "source": [
    "# 4章コードの補足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed67ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.layers import Embedding, SigmoidWithLoss\n",
    "from common.util import create_contexts_target, most_similar, analogy\n",
    "from cbow import CBOW\n",
    "from negative_sampling_layer import EmbeddingDot, UnigramSampler, NegativeSamplingLoss\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb7b0a",
   "metadata": {},
   "source": [
    "# negative_sampling_layer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df0c53",
   "metadata": {},
   "source": [
    "## EmbeddingDot\n",
    "- CBOWモデルの多値分類を二値分類で近似するときに，中間層->出力層の処理を担うレイヤ．\n",
    "- 正解単語のEmbedding，その単語ベクトルと中間層の値の内積を実行する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e2479",
   "metadata": {},
   "source": [
    "### 初期化 init\n",
    "- 引数として重みWを受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = np.random.rand(10, 3)  # 出力層側の重み. 語彙数10, word_vec_size=3の想定\n",
    "embed = Embedding(W_out)  # Embeddingレイヤを生成\n",
    "grads = embed.grads  # Embeddingレイヤの勾配を保持\n",
    "cache = None  # backwardで使う値をfoward時に保持する変数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993b6c5",
   "metadata": {},
   "source": [
    "### forward\n",
    "- 引数の h は中間層のニューロン，idx は正解単語IDの配列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc21443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中間層 h: \n",
      " [[0.60213397 0.3244875  0.01511347]\n",
      " [0.38112923 0.60450997 0.19501062]\n",
      " [0.765577   0.08099657 0.84591937]\n",
      " [0.86757794 0.46484668 0.27737035]\n",
      " [0.29186908 0.8938126  0.72316529]]\n",
      "正解単語ID idx: \n",
      " [0 1 2 0 5]\n"
     ]
    }
   ],
   "source": [
    "h = np.random.rand(5, 3)  # 中間層のニューロン. batch_size=5, word_vec_size=3 の想定．\n",
    "idx = np.array([0, 1, 2, 0, 5])  # 正解の単語ID\n",
    "print(f'中間層 h: \\n {h}')\n",
    "print(f'正解単語ID idx: \\n {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bd3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out: \n",
      " [[0.81594894 0.3513875  0.67716314]\n",
      " [0.86200566 0.51351622 0.04141958]\n",
      " [0.67922469 0.47133728 0.25562047]\n",
      " [0.32895892 0.43082797 0.65220737]\n",
      " [0.70130327 0.53161432 0.9483712 ]\n",
      " [0.22610654 0.2858601  0.1097985 ]\n",
      " [0.67513257 0.38642956 0.62481049]\n",
      " [0.860519   0.99128676 0.62452591]\n",
      " [0.37275783 0.68975826 0.97672205]\n",
      " [0.92619398 0.85919395 0.53245621]]\n",
      "target_W_out: \n",
      " [[0.81594894 0.3513875  0.67716314]\n",
      " [0.86200566 0.51351622 0.04141958]\n",
      " [0.67922469 0.47133728 0.25562047]\n",
      " [0.81594894 0.3513875  0.67716314]\n",
      " [0.22610654 0.2858601  0.1097985 ]]\n"
     ]
    }
   ],
   "source": [
    "target_W_out = embed.forward(idx)  # 正解単語の重みのみを抜き出す\n",
    "print(f'W_out: \\n {W_out}')\n",
    "print(f'target_W_out: \\n {target_W_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba53e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: \n",
      " [0.61556571 0.64703849 0.77440981 1.05906559 0.40090133]\n"
     ]
    }
   ],
   "source": [
    "out = np.sum(target_W_out * h, axis=1)  # 正解単語の重みと中間層の内積計算\n",
    "print(f'out: \\n {out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8597db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = (h, target_W_out)  # backward用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dc8b0",
   "metadata": {},
   "source": [
    "### backward\n",
    "- 勾配 dout を受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd8a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dout: \n",
      " [0.98604428 0.89065471 0.27489795 0.78014773 0.13848396]\n"
     ]
    }
   ],
   "source": [
    "dout = np.random.rand(*out.shape)\n",
    "print(f'dout: \\n {dout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556ff8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, target_W_out = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bedc588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped dout: \n",
      " [[0.98604428]\n",
      " [0.89065471]\n",
      " [0.27489795]\n",
      " [0.78014773]\n",
      " [0.13848396]]\n"
     ]
    }
   ],
   "source": [
    "dout = dout.reshape(dout.shape[0], 1)  # 二次元に変換\n",
    "print(f'reshaped dout: \\n {dout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ec5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtarget_W_out: \n",
      " [[0.59373076 0.31995904 0.01490255]\n",
      " [0.33945455 0.53840965 0.17368713]\n",
      " [0.21045555 0.02226579 0.2325415 ]\n",
      " [0.67683896 0.36264908 0.21638985]\n",
      " [0.04041919 0.12377871 0.1001468 ]]\n"
     ]
    }
   ],
   "source": [
    "dtarget_W_out = dout * h  # 内積の逆伝播\n",
    "print(f'dtarget_W_out: \\n {dtarget_W_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d962e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads: \n",
      " [array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]])]\n",
      "\n",
      "updated grads: \n",
      " [array([[1.27056972, 0.68260812, 0.2312924 ],\n",
      "       [0.33945455, 0.53840965, 0.17368713],\n",
      "       [0.21045555, 0.02226579, 0.2325415 ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.04041919, 0.12377871, 0.1001468 ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ]])]\n"
     ]
    }
   ],
   "source": [
    "print(f'grads: \\n {grads}', end='\\n\\n')\n",
    "embed.backward(dtarget_W_out)  # Embeddingレイヤの逆伝播．勾配を更新．\n",
    "print(f'updated grads: \\n {grads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78110b",
   "metadata": {},
   "source": [
    "***\n",
    "## UnigramSampler\n",
    "- CBOWモデルの高速化の後半部分．\n",
    "- 負例をランダムに抽出して学習させる際の選び方．\n",
    "- コーパス中の単語の出現確率に従ってサンプリングする．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417fd7d5",
   "metadata": {},
   "source": [
    "### 初期化 init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912e9383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3, 1: 2, 3: 2, 4: 2, 5: 2, 2: 1, 6: 1})\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 5, 1, 0, 3, 4, 5, 0, 6])  # コーパスは単語IDのリスト\n",
    "power = 0.75  # 確率に1未満で累乗し，低頻度の単語に下駄をはかせる\n",
    "sample_size = 3  # サンプリングする数\n",
    "\n",
    "# Counterでコーパス中の単語の出現回数をカウントできる\n",
    "counts = collections.Counter()\n",
    "for word_id in corpus:\n",
    "    counts[word_id] += 1\n",
    "print(counts)\n",
    "print(counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122772d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original p: [3. 2. 1. 2. 2. 2. 1.]\n",
      "powerd word_p: [2.27950706 1.68179283 1.         1.68179283 1.68179283 1.68179283\n",
      " 1.        ]\n",
      "\n",
      "p_out: [0.23076923 0.15384615 0.07692308 0.15384615 0.15384615 0.15384615\n",
      " 0.07692308]\n",
      "word_p_out: [0.20710218 0.15279749 0.09085393 0.15279749 0.15279749 0.15279749\n",
      " 0.09085393]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(counts)  # 語彙数 = countsの長さ\n",
    "\n",
    "p = np.zeros(vocab_size)  # 語彙数と同じ要素数の配列で確率を保持する\n",
    "\n",
    "# 各単語IDの出現回数を格納\n",
    "for i in range(vocab_size):\n",
    "    p[i] = counts[i]\n",
    "\n",
    "# 出現回数を0.75乗して稀な単語の確率に少し下駄をはかせる\n",
    "word_p = np.power(p, power)\n",
    "print(f'original p: {p}')\n",
    "print(f'powerd word_p: {word_p}', end='\\n\\n')\n",
    "\n",
    "# np.sum(p) = 単語数 で割って確率にする\n",
    "p /= np.sum(p)\n",
    "word_p /= np.sum(word_p)\n",
    "print(f'p_out: {p}')\n",
    "print(f'word_p_out: {word_p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392674a",
   "metadata": {},
   "source": [
    "### get_negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5784c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットを作る\n",
    "window_size = 1\n",
    "contexts, target = create_contexts_target(corpus, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964407c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf07d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "negative_sample = np.zeros((batch_size, sample_size), dtype=np.int32)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c3f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    p = word_p.copy()  # 確率を取得\n",
    "    target_idx = target[i]  # ターゲットを保持\n",
    "    p[target_idx] = 0  # ターゲットの確率は0\n",
    "    p /= p.sum()  # ターゲットを除いて確率を再計算\n",
    "    negative_sample[i, :] = np.random.choice(vocab_size, size=sample_size, replace=True, p=word_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1abc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5 3]\n",
      " [4 0 6]\n",
      " [4 5 3]\n",
      " [3 1 6]\n",
      " [2 1 5]\n",
      " [5 5 2]\n",
      " [4 0 3]\n",
      " [4 4 0]\n",
      " [3 5 3]\n",
      " [2 3 2]\n",
      " [5 2 4]]\n"
     ]
    }
   ],
   "source": [
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428a82d",
   "metadata": {},
   "source": [
    "***\n",
    "## NegativeSamplingLoss\n",
    "- EmbeddingDotレイヤ，UnigramSamplerレイヤ，SigmoidWithLossレイヤの組み合わせ\n",
    "- forwardでは中間層のニューロンとターゲットから損失関数を計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ffbf2",
   "metadata": {},
   "source": [
    "### 初期化 init\n",
    "- EmbeddingDotレイヤに使う重みW，UnigramSamplerで確率計算に使うcorpus，指数powerとsample_sizeを引数として受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e4eee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 初期化で使う引数 ===\n",
      "W_out: \n",
      " [[0.81594894 0.3513875  0.67716314]\n",
      " [0.86200566 0.51351622 0.04141958]\n",
      " [0.67922469 0.47133728 0.25562047]\n",
      " [0.32895892 0.43082797 0.65220737]\n",
      " [0.70130327 0.53161432 0.9483712 ]\n",
      " [0.22610654 0.2858601  0.1097985 ]\n",
      " [0.67513257 0.38642956 0.62481049]\n",
      " [0.860519   0.99128676 0.62452591]\n",
      " [0.37275783 0.68975826 0.97672205]\n",
      " [0.92619398 0.85919395 0.53245621]]\n",
      "\n",
      "corpus: [0 1 2 3 4 5 1 0 3 4 5 0 6]\n",
      "\n",
      "power: 0.75\n",
      "\n",
      "sample_size: 3\n",
      "\n",
      "=========================\n",
      "contexts: \n",
      " [[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 5]\n",
      " [4 1]\n",
      " [5 0]\n",
      " [1 3]\n",
      " [0 4]\n",
      " [3 5]\n",
      " [4 0]\n",
      " [5 6]]\n",
      "\n",
      "target: \n",
      " [1 2 3 4 5 1 0 3 4 5 0]\n",
      "\n",
      "h: \n",
      " [[0.84099153 0.6989471  0.87519668]\n",
      " [0.92397098 0.10253556 0.81535091]\n",
      " [0.13014711 0.42569723 0.05016932]\n",
      " [0.70655612 0.70709987 0.04225071]\n",
      " [0.32714375 0.53023889 0.19994336]\n",
      " [0.61099243 0.12724178 0.35569203]\n",
      " [0.66099799 0.6523338  0.24827553]\n",
      " [0.07977251 0.93706037 0.54127965]\n",
      " [0.31805531 0.94930844 0.34156912]\n",
      " [0.42580861 0.77402708 0.58595818]\n",
      " [0.75548574 0.34903207 0.32401671]]\n"
     ]
    }
   ],
   "source": [
    "print('=== 初期化で使う引数 ===')\n",
    "print(f'W_out: \\n {W_out}', end='\\n\\n')  # 出力側の単語ベクトルとなる重み．語彙数 x word_vec_size\n",
    "print(f'corpus: {corpus}', end='\\n\\n')  # corpusは単語IDの配列\n",
    "power = 0.75\n",
    "print(f'power: {power}', end='\\n\\n')\n",
    "sample_size = 3\n",
    "print(f'sample_size: {sample_size}', end='\\n\\n')\n",
    "\n",
    "print('=========================')\n",
    "print(f'contexts: \\n {contexts}', end='\\n\\n')  # contextsは単語IDの二次元配列\n",
    "print(f'target: \\n {target}', end='\\n\\n')  # targetは単語IDの配列\n",
    "batch_size = target.shape[0]\n",
    "h = np.random.rand(batch_size, 3)   # 中間層のニューロン\n",
    "print(f'h: \\n {h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d71326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = UnigramSampler(corpus, power, sample_size)  # UnigramSampler初期化\n",
    "loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]  # 負例 + 1(正例)だけ必要\n",
    "embed_dot_layers = [EmbeddingDot(W_out) for _ in range(sample_size + 1)]  # 負例 + 1(正例)だけ必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a4c20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dot_layersのパラメータと勾配をリストにまとめる\n",
    "params, grads = [], []\n",
    "for layer in embed_dot_layers:\n",
    "    params.append(layer.params)\n",
    "    grads.append(layer.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2f00104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重みと勾配はEmbeddingDotレイヤの数 = 負例 + 1(正例)\n",
      "\n",
      "length of params: 4\n",
      "length of grads : 4\n"
     ]
    }
   ],
   "source": [
    "print('重みと勾配はEmbeddingDotレイヤの数 = 負例 + 1(正例)', end='\\n\\n')\n",
    "print(f'length of params: {len(params)}')\n",
    "print(f'length of grads : {len(grads)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfd421",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a79d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: \n",
      " [1 2 3 4 5 1 0 3 4 5 0]\n",
      "batch_size: \n",
      " 11\n"
     ]
    }
   ],
   "source": [
    "batch_size = target.shape[0]\n",
    "print(f'target: \\n {target}')\n",
    "print(f'batch_size: \\n {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28fb7ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_sample: \n",
      " [[3 4 6]\n",
      " [4 0 3]\n",
      " [1 4 5]\n",
      " [6 1 0]\n",
      " [0 6 1]\n",
      " [5 2 4]\n",
      " [4 3 1]\n",
      " [5 0 4]\n",
      " [0 3 6]\n",
      " [6 1 3]\n",
      " [3 6 2]]\n"
     ]
    }
   ],
   "source": [
    "# 負例(targetではない単語)をコーパス中の確率に応じてサンプリングする\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(f'negative_sample: \\n {negative_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92ac72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正例のフォワード\n",
    "# embed_dot_layersとloss_layersの0番目を正例のレイヤとしている\n",
    "score = embed_dot_layers[0].forward(h, target)  # EmbeddingDotレイヤのforward\n",
    "correct_label = np.ones(batch_size, dtype=np.int32)  # 正例は正解ラベルとして1を渡す\n",
    "loss = loss_layers[0].forward(score, correct_label)  # SigmoidWithLossレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88709172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負例のフォワード\n",
    "# embed_dot_layersとloss_layersの1番目以降を負例のレイヤとしている\n",
    "negative_label = np.zeros(batch_size, dtype=np.int32)  # 負例は正解ラベルとして0を渡す\n",
    "for i in range(sample_size):\n",
    "    negative_target = negative_sample[:, i]  # サンプルサイズの回数ループ．バッチ処理．\n",
    "    score = embed_dot_layers[i+1].forward(h, negative_target)  # EmbeddingDotレイヤのforward\n",
    "    loss += loss_layers[i+1].forward(score, negative_label)  # SigmoidWithLossレイヤのforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "603b1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: \n",
      " 3.9742251550519954\n"
     ]
    }
   ],
   "source": [
    "print(f'loss: \\n {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75116810",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8b412a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6646410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh: \n",
      " [[0.1071578  0.08777061 0.16341233]\n",
      " [0.11379134 0.0798213  0.15409554]\n",
      " [0.08133077 0.05266617 0.03263724]\n",
      " [0.13266799 0.06649753 0.06009547]\n",
      " [0.12753974 0.0611703  0.07363762]\n",
      " [0.06755035 0.05892495 0.07911477]\n",
      " [0.10196327 0.08592481 0.088246  ]\n",
      " [0.10064995 0.06066293 0.09335572]\n",
      " [0.09756289 0.06037947 0.09969638]\n",
      " [0.11082107 0.074393   0.08113921]\n",
      " [0.08386053 0.07031631 0.0769547 ]]\n",
      "h : \n",
      " [[0.84099153 0.6989471  0.87519668]\n",
      " [0.92397098 0.10253556 0.81535091]\n",
      " [0.13014711 0.42569723 0.05016932]\n",
      " [0.70655612 0.70709987 0.04225071]\n",
      " [0.32714375 0.53023889 0.19994336]\n",
      " [0.61099243 0.12724178 0.35569203]\n",
      " [0.66099799 0.6523338  0.24827553]\n",
      " [0.07977251 0.93706037 0.54127965]\n",
      " [0.31805531 0.94930844 0.34156912]\n",
      " [0.42580861 0.77402708 0.58595818]\n",
      " [0.75548574 0.34903207 0.32401671]]\n"
     ]
    }
   ],
   "source": [
    "# dhを初期化\n",
    "dh = 0\n",
    "\n",
    "# SigmoidWithLossレイヤ->EmbeddingDotレイヤの順にbackward\n",
    "for l0, l1 in zip(loss_layers, embed_dot_layers):\n",
    "    dscore = l0.backward(dout)\n",
    "    dh += l1.backward(dscore)  # hの順伝播はリピートノードなので，逆伝播では足し合わせる\n",
    "    \n",
    "print(f'dh: \\n {dh}')\n",
    "print(f'h : \\n {h}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0868a6",
   "metadata": {},
   "source": [
    "***\n",
    "# cbow.py\n",
    "### CBOW\n",
    "- 改良版CBOWモデル．\n",
    "- EmbeddingレイヤとNegativeSamplingLossレイヤを使う．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113b8d2",
   "metadata": {},
   "source": [
    "### 初期化 init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "347adda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===引数===\n",
      "vocab_size: 7\n",
      "hidden_size: 3\n",
      "corpus: [0 1 2 3 4 5 1 0 3 4 5 0 6]\n",
      "window_size: 2\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 3  # embeddingした単語ベクトルの次元\n",
    "window_size = 2  # targetの両側いくつか？\n",
    "\n",
    "print('===引数===')\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "print(f'hidden_size: {hidden_size}')\n",
    "print(f'corpus: {corpus}')\n",
    "print(f'window_size: {window_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb77a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重みの初期化\n",
    "V, H = vocab_size, hidden_size\n",
    "W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "W_out = 0.01 * np.random.randn(V, H).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e424770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レイヤの生成\n",
    "in_layers = []\n",
    "for i in range(window_size*2):\n",
    "    layer = Embedding(W_in)\n",
    "    in_layers.append(layer)\n",
    "ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1635e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重みと勾配をリストで管理する\n",
    "layers = in_layers + [ns_loss]\n",
    "params, grads = [], []\n",
    "for layer in layers:\n",
    "    params.append(layer.params)\n",
    "    grads.append(layer.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "483146b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス変数に単語の分散表現を設定\n",
    "word_vecs = W_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42d5bc",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3453452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts\n",
      "[[0 1 3 4]\n",
      " [1 2 4 5]\n",
      " [2 3 5 1]\n",
      " [3 4 1 0]\n",
      " [4 5 0 3]\n",
      " [5 1 3 4]\n",
      " [1 0 4 5]\n",
      " [0 3 5 0]\n",
      " [3 4 0 6]]\n",
      "targets\n",
      "[2 3 4 5 1 0 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, targets = create_contexts_target(corpus, window_size)\n",
    "print('contexts')\n",
    "print(contexts)\n",
    "print('targets')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60eb6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0794453620910645\n"
     ]
    }
   ],
   "source": [
    "h = 0\n",
    "for i, layer in enumerate(in_layers):\n",
    "    h += layer.forward(contexts[:, i])  # Embeddingレイヤの順伝播\n",
    "h *= 1 / len(in_layers)  # コンテキスト全体の平均を中間層のニューロンとする\n",
    "loss = ns_loss.forward(h, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8dc6d",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d57cd8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= before backward =========\n",
      "grad of ns_loss\n",
      "[array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32), array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32), array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)]\n",
      "grad of in_layer_0\n",
      "[array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)]\n",
      "========= after backward =========\n",
      "grad of ns_loss\n",
      "[array([[-3.7824440e-05,  7.5002678e-04,  6.0634833e-05],\n",
      "       [ 2.4280272e-04,  5.2335463e-04, -3.9331842e-04],\n",
      "       [-4.5670200e-05,  3.8197232e-04,  1.3498714e-05],\n",
      "       [-2.6116133e-04,  1.3214612e-03, -3.6519571e-04],\n",
      "       [ 8.0582354e-04,  8.8308845e-04, -7.2851323e-04],\n",
      "       [-3.1433523e-05,  3.3593707e-04, -1.7052324e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32), array([[-5.12618979e-04, -2.93048593e-04,  6.83348218e-04],\n",
      "       [ 8.57418927e-05, -6.49787718e-04,  1.20999495e-04],\n",
      "       [ 3.78272161e-05, -7.50081846e-04, -6.06392859e-05],\n",
      "       [ 4.56701018e-05, -3.81971535e-04, -1.34987031e-05],\n",
      "       [ 4.56716916e-05, -3.81984806e-04, -1.34991551e-05],\n",
      "       [-3.74832074e-04, -1.73887075e-03,  7.13171321e-04],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]], dtype=float32), array([[ 4.5671790e-05, -3.8198562e-04, -1.3499184e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 2.6114873e-04, -1.3214037e-03,  3.6517810e-04],\n",
      "       [-7.6796382e-04, -1.6330500e-03,  6.6783675e-04],\n",
      "       [ 3.1432181e-05, -3.3592619e-04,  1.7052846e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [-2.4281249e-04, -5.2337564e-04,  3.9333422e-04]], dtype=float32)]\n",
      "grad of in_layer_0\n",
      "[array([[-4.6603804e-04,  2.3579969e-04, -6.0253742e-04],\n",
      "       [ 4.6111649e-04, -5.3014705e-04,  5.2168104e-04],\n",
      "       [-1.8058528e-04,  2.3172576e-04, -1.2799953e-04],\n",
      "       [-3.6342206e-04, -8.9797926e-05, -6.7266653e-04],\n",
      "       [ 1.4407167e-04,  1.4768275e-04, -2.3625986e-05],\n",
      "       [-4.6782017e-05,  1.0302244e-05, -7.8462268e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print('========= before backward =========')\n",
    "print('grad of ns_loss')\n",
    "print(grads[-1])\n",
    "\n",
    "print('grad of in_layer_0')\n",
    "print(grads[0])\n",
    "\n",
    "dout = 1\n",
    "dout = ns_loss.backward(dout)\n",
    "dout *= 1 / len(in_layers)  # コンテキスト一つ分の損失にする\n",
    "for layer in in_layers:\n",
    "    layer.backward(dout)\n",
    "    \n",
    "print('========= after backward =========')\n",
    "print('grad of ns_loss')\n",
    "print(grads[-1])\n",
    "\n",
    "print('grad of in_layer_0')\n",
    "print(grads[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607ec95",
   "metadata": {},
   "source": [
    "***\n",
    "# train.py\n",
    "- 改良版CBOWモデルの学習コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fef64e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ設定\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a6defe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76f819c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル生成\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "469a762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: <class 'cbow.CBOW'>\n",
      "num of in_layers: 10\n",
      "num of params of a in_layer: 1\n",
      "num of params of ns_loss: 6\n",
      "num of params: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'model: {type(model)}')\n",
    "print(f'num of in_layers: {len(model.in_layers)}')  # window_size=5なのでin_layerは10個\n",
    "print(f'num of params of a in_layer: {len(model.in_layers[0].params)}')  # Embeddingレイヤは重み1個\n",
    "# NegativeSamplingLossレイヤのEmbeddingDotレイヤに重みが1個\n",
    "# sample_sizeのdefault値5+1(正例分)\n",
    "print(f'num of params of ns_loss: {len(model.ns_loss.params)}')\n",
    "print(f'num of params: {len(model.params)}')  # 計16個のパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b2ffc89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "# 学習はgpu環境でないとキツイ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc962017",
   "metadata": {},
   "source": [
    "***\n",
    "# eval.py\n",
    "- CBOWモデルによる単語の分散表現の評価\n",
    "- 学習済みパラメータを読み込んで使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "466dbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "id_to_word = params['id_to_word']\n",
    "word_to_id = params['word_to_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "387331e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.6103515625\n",
      " someone: 0.59130859375\n",
      " i: 0.55419921875\n",
      " something: 0.48974609375\n",
      " anyone: 0.47314453125\n",
      "\n",
      "[query] year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query] car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query] toyota\n",
      " ford: 0.55078125\n",
      " instrumentation: 0.509765625\n",
      " mazda: 0.49365234375\n",
      " bethlehem: 0.47509765625\n",
      " nissan: 0.474853515625\n"
     ]
    }
   ],
   "source": [
    "# 近い単語\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "674de37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.16015625\n",
      " veto: 4.9296875\n",
      " ounce: 4.69140625\n",
      " earthquake: 4.6328125\n",
      " successor: 4.609375\n"
     ]
    }
   ],
   "source": [
    "# アナロジー問題\n",
    "analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6d723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] take:took = go:?\n",
      " went: 4.55078125\n",
      " points: 4.25\n",
      " began: 4.09375\n",
      " comes: 3.98046875\n",
      " oct.: 3.90625\n"
     ]
    }
   ],
   "source": [
    "analogy('take', 'took', 'go', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e7bb9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] car:cars = child:?\n",
      " children: 5.21875\n",
      " average: 4.7265625\n",
      " yield: 4.20703125\n",
      " cattle: 4.1875\n",
      " priced: 4.1796875\n"
     ]
    }
   ],
   "source": [
    "analogy('car', 'cars', 'child', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8d2d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] good:better = bad:?\n",
      " more: 6.6484375\n",
      " less: 6.0625\n",
      " rather: 5.21875\n",
      " slower: 4.734375\n",
      " greater: 4.671875\n"
     ]
    }
   ],
   "source": [
    "analogy('good', 'better', 'bad', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d581913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] small:big = short:?\n",
      " ual: 4.87109375\n",
      " board: 4.48046875\n",
      " trading: 4.44140625\n",
      " dow: 4.125\n",
      " nasdaq: 3.8203125\n"
     ]
    }
   ],
   "source": [
    "analogy('small', 'big', 'short', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d077532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] small:big = short:?\n",
      " ual: 4.87109375\n",
      " board: 4.48046875\n",
      " trading: 4.44140625\n",
      " dow: 4.125\n",
      " nasdaq: 3.8203125\n"
     ]
    }
   ],
   "source": [
    "analogy('small', 'big', 'short', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
