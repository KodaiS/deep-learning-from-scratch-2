{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd44312",
   "metadata": {},
   "source": [
    "# 4章コードの補足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed67ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import collections\n",
    "from common.layers import Embedding, SigmoidWithLoss\n",
    "from common.util import create_contexts_target\n",
    "from negative_sampling_layer import EmbeddingDot, UnigramSampler, NegativeSamplingLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb7b0a",
   "metadata": {},
   "source": [
    "# negative_sampling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df0c53",
   "metadata": {},
   "source": [
    "***\n",
    "## EmbeddingDot\n",
    "- CBOWモデルの多値分類を二値分類で近似するときに，中間層->出力層の処理を担うレイヤ．\n",
    "- 正解単語のEmbedding，その単語ベクトルと中間層の値の内積を実行する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e2479",
   "metadata": {},
   "source": [
    "### 初期化 init\n",
    "- 引数として重みWを受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = np.random.rand(10, 3)  # 出力層側の重み. 語彙数10, word_vec_size=3の想定\n",
    "embed = Embedding(W_out)  # Embeddingレイヤを生成\n",
    "grads = embed.grads  # Embeddingレイヤの勾配を保持\n",
    "cache = None  # backwardで使う値をfoward時に保持する変数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993b6c5",
   "metadata": {},
   "source": [
    "### forward\n",
    "- 引数の h は中間層のニューロン，idx は正解単語IDの配列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc21443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中間層 h: \n",
      " [[0.69131206 0.12199617 0.83288524]\n",
      " [0.30376944 0.90889519 0.86545502]\n",
      " [0.42530876 0.15244514 0.45250241]\n",
      " [0.01374559 0.8685118  0.1072651 ]\n",
      " [0.76555321 0.96973996 0.70623601]]\n",
      "正解単語ID idx: \n",
      " [0 1 2 0 5]\n"
     ]
    }
   ],
   "source": [
    "h = np.random.rand(5, 3)  # 中間層のニューロン. batch_size=5, word_vec_size=3 の想定．\n",
    "idx = np.array([0, 1, 2, 0, 5])  # 正解の単語ID\n",
    "print(f'中間層 h: \\n {h}')\n",
    "print(f'正解単語ID idx: \\n {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bd3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out: \n",
      " [[0.57095902 0.88578706 0.16310808]\n",
      " [0.28086643 0.98137859 0.06785615]\n",
      " [0.97002936 0.6671     0.31561775]\n",
      " [0.60742155 0.42754972 0.10767568]\n",
      " [0.13168318 0.67683056 0.09379592]\n",
      " [0.83004069 0.80925757 0.4974298 ]\n",
      " [0.08318498 0.57717464 0.72581702]\n",
      " [0.52045138 0.3135339  0.77926451]\n",
      " [0.98103537 0.25710681 0.93057164]\n",
      " [0.31986886 0.64133897 0.36636446]]\n",
      "target_W_out: \n",
      " [[0.57095902 0.88578706 0.16310808]\n",
      " [0.28086643 0.98137859 0.06785615]\n",
      " [0.97002936 0.6671     0.31561775]\n",
      " [0.57095902 0.88578706 0.16310808]\n",
      " [0.83004069 0.80925757 0.4974298 ]]\n"
     ]
    }
   ],
   "source": [
    "target_W_out = embed.forward(idx)  # 正解単語の重みのみを抜き出す\n",
    "print(f'W_out: \\n {W_out}')\n",
    "print(f'target_W_out: \\n {target_W_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba53e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: \n",
      " [0.6386238  1.03601536 0.65707593 0.79466049 1.77151255]\n"
     ]
    }
   ],
   "source": [
    "out = np.sum(target_W_out * h, axis=1)  # 正解単語の重みと中間層の内積計算\n",
    "print(f'out: \\n {out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8597db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = (h, target_W_out)  # backward用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dc8b0",
   "metadata": {},
   "source": [
    "### backward\n",
    "- 勾配 dout を受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd8a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dout: \n",
      " [0.85448046 0.82235243 0.18498158 0.72358886 0.38614443]\n"
     ]
    }
   ],
   "source": [
    "dout = np.random.rand(*out.shape)\n",
    "print(f'dout: \\n {dout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556ff8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, target_W_out = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bedc588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped dout: \n",
      " [[0.85448046]\n",
      " [0.82235243]\n",
      " [0.18498158]\n",
      " [0.72358886]\n",
      " [0.38614443]]\n"
     ]
    }
   ],
   "source": [
    "dout = dout.reshape(dout.shape[0], 1)  # 二次元に変換\n",
    "print(f'reshaped dout: \\n {dout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ec5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtarget_W_out: \n",
      " [[0.59071265 0.10424335 0.71168416]\n",
      " [0.24980554 0.74743217 0.71170904]\n",
      " [0.07867429 0.02819954 0.08370461]\n",
      " [0.00994615 0.62844546 0.07761583]\n",
      " [0.2956141  0.37445968 0.2727091 ]]\n"
     ]
    }
   ],
   "source": [
    "dtarget_W_out = dout * h  # 内積の逆伝播\n",
    "print(f'dtarget_W_out: \\n {dtarget_W_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d962e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads: \n",
      " [array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]])]\n",
      "\n",
      "updated grads: \n",
      " [array([[0.6006588 , 0.73268881, 0.78929999],\n",
      "       [0.24980554, 0.74743217, 0.71170904],\n",
      "       [0.07867429, 0.02819954, 0.08370461],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.2956141 , 0.37445968, 0.2727091 ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ]])]\n"
     ]
    }
   ],
   "source": [
    "print(f'grads: \\n {grads}', end='\\n\\n')\n",
    "embed.backward(dtarget_W_out)  # Embeddingレイヤの逆伝播．勾配を更新．\n",
    "print(f'updated grads: \\n {grads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78110b",
   "metadata": {},
   "source": [
    "***\n",
    "## UnigramSampler\n",
    "- CBOWモデルの高速化の後半部分．\n",
    "- 負例をランダムに抽出して学習させる際の選び方．\n",
    "- コーパス中の単語の出現確率に従ってサンプリングする．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417fd7d5",
   "metadata": {},
   "source": [
    "### 初期化 init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912e9383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 4, 0: 3, 3: 2, 2: 1, 4: 1, 5: 1})\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array([1, 1, 3, 2, 1, 1, 0, 3, 4, 5, 0, 0])  # コーパスは単語IDのリスト\n",
    "power = 0.75  # 確率に1未満で累乗し，低頻度の単語に下駄をはかせる\n",
    "sample_size = 3  # サンプリングする数\n",
    "\n",
    "# Counterでコーパス中の単語の出現回数をカウントできる\n",
    "counts = collections.Counter()\n",
    "for word_id in corpus:\n",
    "    counts[word_id] += 1\n",
    "print(counts)\n",
    "print(counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122772d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original p: [3. 4. 1. 2. 1. 1.]\n",
      "powerd word_p: [2.27950706 2.82842712 1.         1.68179283 1.         1.        ]\n",
      "\n",
      "p_out: [0.25       0.33333333 0.08333333 0.16666667 0.08333333 0.08333333]\n",
      "word_p_out: [0.23284685 0.28891787 0.10214789 0.1717916  0.10214789 0.10214789]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(counts)  # 語彙数 = countsの長さ\n",
    "\n",
    "p = np.zeros(vocab_size)  # 語彙数と同じ要素数の配列で確率を保持する\n",
    "\n",
    "# 各単語IDの出現回数を格納\n",
    "for i in range(vocab_size):\n",
    "    p[i] = counts[i]\n",
    "\n",
    "# 出現回数を0.75乗して稀な単語の確率に少し下駄をはかせる\n",
    "word_p = np.power(p, power)\n",
    "print(f'original p: {p}')\n",
    "print(f'powerd word_p: {word_p}', end='\\n\\n')\n",
    "\n",
    "# np.sum(p) = 単語数 で割って確率にする\n",
    "p /= np.sum(p)\n",
    "word_p /= np.sum(word_p)\n",
    "print(f'p_out: {p}')\n",
    "print(f'word_p_out: {word_p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392674a",
   "metadata": {},
   "source": [
    "### get_negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5784c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットを作る\n",
    "window_size = 1\n",
    "contexts, target = create_contexts_target(corpus, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964407c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf07d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "negative_sample = np.zeros((batch_size, sample_size), dtype=np.int32)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c3f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    p = word_p.copy()  # 確率を取得\n",
    "    target_idx = target[i]  # ターゲットを保持\n",
    "    p[target_idx] = 0  # ターゲットの確率は0\n",
    "    p /= p.sum()  # ターゲットを除いて確率を再計算\n",
    "    negative_sample[i, :] = np.random.choice(vocab_size, size=sample_size, replace=True, p=word_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1abc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 3]\n",
      " [0 2 1]\n",
      " [4 1 1]\n",
      " [1 1 0]\n",
      " [1 1 5]\n",
      " [3 1 0]\n",
      " [1 3 3]\n",
      " [0 0 1]\n",
      " [1 4 3]\n",
      " [4 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428a82d",
   "metadata": {},
   "source": [
    "***\n",
    "## NegativeSamplingLoss\n",
    "- EmbeddingDotレイヤ，UnigramSamplerレイヤ，SigmoidWithLossレイヤの組み合わせ\n",
    "- forwardでは中間層のニューロンとターゲットから損失関数を計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ffbf2",
   "metadata": {},
   "source": [
    "### 初期化 init\n",
    "- EmbeddingDotレイヤに使う重みW，UnigramSamplerで確率計算に使うcorpus，指数powerとsample_sizeを引数として受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4e4eee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out: \n",
      " [[0.57095902 0.88578706 0.16310808]\n",
      " [0.28086643 0.98137859 0.06785615]\n",
      " [0.97002936 0.6671     0.31561775]\n",
      " [0.60742155 0.42754972 0.10767568]\n",
      " [0.13168318 0.67683056 0.09379592]\n",
      " [0.83004069 0.80925757 0.4974298 ]\n",
      " [0.08318498 0.57717464 0.72581702]\n",
      " [0.52045138 0.3135339  0.77926451]\n",
      " [0.98103537 0.25710681 0.93057164]\n",
      " [0.31986886 0.64133897 0.36636446]]\n",
      "\n",
      "corpus: \n",
      " [1 1 3 2 1 1 0 3 4 5 0 0]\n",
      "\n",
      "contexts: \n",
      " [[1 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 3]\n",
      " [0 4]\n",
      " [3 5]\n",
      " [4 0]\n",
      " [5 0]]\n",
      "\n",
      "target: \n",
      " [1 3 2 1 1 0 3 4 5 0]\n",
      "\n",
      "h: \n",
      " [[0.57639247 0.13477406 0.72099445]\n",
      " [0.88410564 0.75463007 0.35209841]\n",
      " [0.05596927 0.21241603 0.46759997]\n",
      " [0.28323451 0.63438585 0.92653796]\n",
      " [0.42567917 0.55794762 0.0647181 ]\n",
      " [0.57546497 0.06877354 0.64367143]\n",
      " [0.60777673 0.20228706 0.72870647]\n",
      " [0.71337789 0.62738251 0.84823134]\n",
      " [0.49099186 0.60245709 0.39911593]\n",
      " [0.26432596 0.49271598 0.0830015 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'W_out: \\n {W_out}', end='\\n\\n')  # 出力側の単語ベクトルとなる重み．語彙数 x word_vec_size\n",
    "print(f'corpus: \\n {corpus}', end='\\n\\n')  # corpusは単語IDの配列\n",
    "print(f'contexts: \\n {contexts}', end='\\n\\n')  # contextsは単語IDの二次元配列\n",
    "print(f'target: \\n {target}', end='\\n\\n')  # targetは単語IDの配列\n",
    "h = np.random.rand(10, 3)   # 中間層のニューロン\n",
    "print(f'h: \\n {h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d71326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = 0.75  # UnigramSamplerの確率計算に使用．稀な単語の確率に下駄をはかせる．\n",
    "sample_size = 3  # 負例として3単語をサンプリングする設定\n",
    "sampler = UnigramSampler(corpus, power, sample_size)  # UnigramSampler初期化\n",
    "loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]  # 負例 + 1(正例)だけ必要\n",
    "embed_dot_layers = [EmbeddingDot(W_out) for _ in range(sample_size + 1)]  # 負例 + 1(正例)だけ必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a4c20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dot_layersのパラメータと勾配をリストにまとめる\n",
    "params, grads = [], []\n",
    "for layer in embed_dot_layers:\n",
    "    params.append(layer.params)\n",
    "    grads.append(layer.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2f00104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of params: 4\n",
      "length of grads : 4\n"
     ]
    }
   ],
   "source": [
    "# 重みと勾配はEmbeddingDotレイヤの数だけある\n",
    "print(f'length of params: {len(params)}')\n",
    "print(f'length of grads : {len(grads)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfd421",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a79d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "batch_size = target.shape[0]\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28fb7ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 2]\n",
      " [0 5 1]\n",
      " [5 1 0]\n",
      " [0 3 5]\n",
      " [0 2 3]\n",
      " [1 2 3]\n",
      " [4 0 1]\n",
      " [3 5 2]\n",
      " [1 4 3]\n",
      " [4 2 5]]\n"
     ]
    }
   ],
   "source": [
    "# 負例(targetではない単語)をコーパス中の確率に応じてサンプリングする\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92ac72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正例のフォワード\n",
    "score = embed_dot_layers[0].forward(h, target)  # EmbeddingDotレイヤのforward\n",
    "correct_label = np.ones(batch_size, dtype=np.int32)  # SigmoidWithLossに入れる正解ラベル\n",
    "loss = loss_layers[0].forward(score, correct_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "88709172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4310707531655287"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b1fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
