{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd9f83f",
   "metadata": {},
   "source": [
    "# 3.1 推論ベースの手法とニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb20d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "from common.util import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e008b",
   "metadata": {},
   "source": [
    "単語IDが0の単語を全結合層で変換する例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5247ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7)\n",
      "[[1.29974161 0.95047189 0.73161655]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "print(c.shape)  # ミニバッチ処理を考えて2次元にしている\n",
    "W = np.random.randn(7, 3)\n",
    "h = np.dot(c, W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a127a",
   "metadata": {},
   "source": [
    "MatMulレイヤを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a164d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.29974161 0.95047189 0.73161655]]\n"
     ]
    }
   ],
   "source": [
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a3903",
   "metadata": {},
   "source": [
    "# 3.2 シンプルなword2vec (CBOWモデル)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a8b82",
   "metadata": {},
   "source": [
    "## CBOWモデルの推論処理\n",
    "**入力層 - 中間層 - 出力層** という単純なモデルを考える\n",
    "- 入力層と出力層のニューロンの数は単語ベクトルの要素数\n",
    "- 入力層の数はコンテキストとして与える単語数\n",
    "- 全ての入力層から中間層への変換は重み$\\mathrm{W_{in}}$の全結合層によって，中間層から出力層への変換は重み$\\mathrm{W_{out}}$の全結合層によって行われる\n",
    "- 中間層は各入力層の全結合による変換後の値を平均したもの\n",
    "- 出力層の値は各単語のスコア．Softmax関数を適用すると各単語の確率になる．\n",
    "- 学習後の重み$\\mathrm{W_{in}}, \\mathrm{W_{out}}$が単語の分散表現になる．中間層のニューロンの数を入力層よりも少なくすることで，密なベクトルが得られる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9441689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.01604295  0.65472736 -0.71989758 -0.7050117   1.11093053 -0.42000925\n",
      "   2.91196432]]\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータ\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 重み初期化\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# レイヤ生成\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# forward\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "# スコア出力\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd379433",
   "metadata": {},
   "source": [
    "## CBOWモデルの学習\n",
    "ネットワークから出力されるスコアにSoftmax関数を適用することで確率を得ることができる．上の例では，単語IDが0と2の単語と共起する単語を予測するようなタスクを考えている．正解が単語IDが1の単語であれば，s[1]が最も高くなるように重みを調整することになる．  \n",
    "追加する層は\n",
    "- Softmaxレイヤ\n",
    "- Cross Entropy Errorレイヤ\n",
    "\n",
    "の2つ．すでに実装しているSoftmax with Lossレイヤで実装できる．\n",
    "\n",
    "<br/>\n",
    "\n",
    "最終的に利用する単語の分散表現の選択肢は次の3つ\n",
    "- 入力側の重み: 行ベクトルが各単語の分散表現に対応．こちらだけを使うのが最もポピュラーな方法．\n",
    "- 出力側の重み: 列ベクトルが各単語の分散表現に対応．\n",
    "- 入出力の重み: 両方を足し合わせるなどの方法\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282effd7",
   "metadata": {},
   "source": [
    "# 3.3 学習データの準備\n",
    "## コンテキストとターゲット\n",
    "**コーパス &rarr; (コンテキスト & ターゲット)** という処理を行う  \n",
    "まずはコーパスを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29ee982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: [0 1 2 3 4 1 5 6]\n",
      "id_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(f'corpus: {corpus}')\n",
    "print(f'id_to_word: {id_to_word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e6694",
   "metadata": {},
   "source": [
    "コンテキストとターゲットを作る関数を実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c4bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''コンテキストとターゲットの作成\n",
    "    :param corpus: コーパス（単語IDのリスト）\n",
    "    :param window_size: ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
    "    :return: tuple (contexts, target)\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]  # 端の単語を除く\n",
    "    contexts = []\n",
    "\n",
    "    # コーパスの両端からwindow_size分を除いてループ\n",
    "    # ターゲットのidxになる\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        # 今のidxから前後window_size分を見てコンテキストとして追加する\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e7e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts:\n",
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "target: [1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(f'contexts:\\n{contexts}')\n",
    "print(f'target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ef006",
   "metadata": {},
   "source": [
    "## one-hot表現への変換\n",
    "NNに入力するためone-hotベクトルに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31da812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''one-hot表現への変換\n",
    "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
    "    :param vocab_size: 語彙数\n",
    "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f8482df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022c59e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0]],\n",
       "\n",
       "       [[0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1]]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33eeb3",
   "metadata": {},
   "source": [
    "# CBOWモデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b65c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477ef8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
